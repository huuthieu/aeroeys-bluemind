{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2074a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Owlv2ForObjectDetection, AutoProcessor\n",
    "import torch\n",
    "\n",
    "owlv2 = Owlv2ForObjectDetection.from_pretrained(\n",
    "    \"google/owlv2-large-patch14-ensemble\", dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(owlv2.name_or_path, use_fast=True)\n",
    "\n",
    "interpolate_pos_encoding = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b4845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import decode_image\n",
    "from pathlib import Path\n",
    "from transformers import TensorType\n",
    "import torch\n",
    "\n",
    "ref_paths = list(Path(\"ref\").glob(\"*_obj.png\"))\n",
    "\n",
    "query_inputs = processor(\n",
    "    query_images=[str(path) for path in ref_paths],\n",
    "    # query_images=[\n",
    "    #     decode_image(path, apply_exif_orientation=True) for path in ref_paths\n",
    "    # ],\n",
    "    return_tensors=TensorType.PYTORCH,\n",
    "    device=owlv2.device,\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    query_feature_maps = owlv2.image_embedder(\n",
    "        query_inputs.query_pixel_values,\n",
    "        interpolate_pos_encoding=interpolate_pos_encoding,\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abb133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"frame_003387_02_15.jpg\"\n",
    "target_inputs = processor(\n",
    "    images=img_path,\n",
    "    return_tensors=TensorType.PYTORCH,\n",
    "    device=owlv2.device,\n",
    ")\n",
    "\n",
    "query_feature_map = query_feature_maps.mean(0)[None]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    target_feature_map = owlv2.image_embedder(\n",
    "        target_inputs.pixel_values, interpolate_pos_encoding=interpolate_pos_encoding\n",
    "    )[0]\n",
    "\n",
    "    batch_size = query_feature_map.size(0)\n",
    "    output_dim = query_feature_map.size(-1)\n",
    "\n",
    "    query_feats = query_feature_map.reshape(batch_size, -1, output_dim)\n",
    "    objectness_logits = owlv2.objectness_predictor(query_feats)\n",
    "    best_box_indices = torch.argmax(objectness_logits, dim=1).view(-1, 1, 1)\n",
    "\n",
    "    query_class_embeds = owlv2.class_predictor(query_feats)[1]\n",
    "    hidden_size = query_class_embeds.size(-1)\n",
    "    # B 1 hidden_size\n",
    "    query_embeds = torch.gather(\n",
    "        query_class_embeds,\n",
    "        dim=1,\n",
    "        index=best_box_indices.expand(-1, 1, hidden_size),\n",
    "    )\n",
    "\n",
    "    target_feats = target_feature_map.reshape(batch_size, -1, output_dim)\n",
    "    pred_boxes = owlv2.box_predictor(\n",
    "        target_feats,\n",
    "        target_feature_map,\n",
    "        interpolate_pos_encoding=interpolate_pos_encoding,\n",
    "    )\n",
    "    pred_scores = torch.sigmoid(owlv2.class_predictor(target_feats, query_embeds)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b78023d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 4 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _box_tensor, _score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pixel_boxes, selected_scores):\n\u001b[32m     23\u001b[39m         _box = _box_tensor.tolist()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         _draw.text(_box[:\u001b[32m2\u001b[39m], \u001b[38;5;28mstr\u001b[39m(\u001b[43m_score\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m     25\u001b[39m         _draw.rectangle(_box)\n\u001b[32m     26\u001b[39m im\n",
      "\u001b[31mRuntimeError\u001b[39m: a Tensor with 4 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.v2.functional import convert_bounding_box_format\n",
    "from torchvision.tv_tensors import BoundingBoxes, BoundingBoxFormat\n",
    "from torchvision.ops import nms\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "score_mask = pred_scores.squeeze(-1) > 0.1\n",
    "selected_boxes = pred_boxes[score_mask]\n",
    "selected_scores = pred_scores[score_mask]\n",
    "\n",
    "with Image.open(img_path) as im:\n",
    "    boxes = BoundingBoxes(\n",
    "        selected_boxes, format=BoundingBoxFormat.CXCYWH, canvas_size=im.size\n",
    "    )\n",
    "    xyxy_boxes = convert_bounding_box_format(boxes, new_format=BoundingBoxFormat.XYXY)\n",
    "\n",
    "    indices = nms(xyxy_boxes, selected_scores.squeeze(), iou_threshold=0.001)\n",
    "    selected_boxes = xyxy_boxes[indices] * max(im.size)\n",
    "    selected_scores = selected_scores[indices]\n",
    "\n",
    "    # selected_boxes = selected_boxes[indices] * max(im.size)\n",
    "    _draw = ImageDraw.Draw(im)\n",
    "    for _box_tensor, _score in zip(pixel_boxes, selected_scores):\n",
    "        _box = _box_tensor.tolist()\n",
    "        _draw.text(_box[:2], str(_score.item()))\n",
    "        _draw.rectangle(_box)\n",
    "im"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
