{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ee1318",
   "metadata": {},
   "source": [
    "# YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda807ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets\n",
    "\n",
    "MODEL_DIR = Path(\"../model_repository\")\n",
    "\n",
    "model_dropdown = ipywidgets.Dropdown(\n",
    "    options=[\n",
    "        \"yoloe-v8l-seg.pt\",\n",
    "        \"yoloe-11l-seg.pt\",\n",
    "        \"yoloe-11l-seg-pf.pt\",\n",
    "        \"yoloe-v8l-seg-pf.pt\",\n",
    "    ],\n",
    "    description=\"YOLOE\",\n",
    ")\n",
    "\n",
    "model_dropdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf778b64",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ultralytics.models.yolo import YOLOE\n",
    "from ultralytics.models.yolo.yoloe import YOLOEVPDetectPredictor\n",
    "\n",
    "model_name = MODEL_DIR / model_dropdown.value\n",
    "yoloe = YOLOE(model_name).eval()\n",
    "is_prompt_free = \"-pf\" in model_name.name\n",
    "\n",
    "ref_paths = list(Path(\"ref\").glob(\"*.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc521634",
   "metadata": {},
   "source": [
    "## Few-shot image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587510fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "predictor = YOLOEVPDetectPredictor(\n",
    "    overrides={\"task\": \"detect\", \"mode\": \"predict\", \"batch\": 1}\n",
    ")\n",
    "predictor.setup_model(yoloe.model)\n",
    "\n",
    "all_vpe = []\n",
    "\n",
    "for ref_path in ref_paths:\n",
    "    path = ref_path.with_name(f\"{ref_path.stem}_obj.png\")\n",
    "    # ref_img = torchvision.io.read_image(ref_path)\n",
    "\n",
    "    bg_path = ref_path.with_name(f\"{ref_path.stem}_bg.png\")\n",
    "    bg_np = cv2.imread(str(bg_path), cv2.IMREAD_GRAYSCALE)\n",
    "    assert bg_np is not None\n",
    "    bg_np = (bg_np > (255 // 2)).astype(np.uint8)\n",
    "\n",
    "    predictor.set_prompts(\n",
    "        {\n",
    "            \"masks\": [bg_np],  # (N, H, W)\n",
    "            \"cls\": np.array([0]),  # [class1, class2, ...]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    vpe = predictor.get_vpe(ref_path)\n",
    "\n",
    "    all_vpe.append(vpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60b015",
   "metadata": {},
   "source": [
    "## Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pe = yoloe.get_text_pe([\"backpack\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b90bddf",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vpe.append(text_pe)\n",
    "avg_vpe = torch.mean(torch.stack(all_vpe), dim=0)\n",
    "yoloe.set_classes([\"obj\"], avg_vpe)\n",
    "\n",
    "results = yoloe.predict(sorted(Path().glob(\"frame*\")), conf=0.0001, iou=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc57d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].show(masks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41087e",
   "metadata": {},
   "source": [
    "# Crop objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad0e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchvision.ops import masks_to_boxes, roi_align\n",
    "\n",
    "result = results[0]\n",
    "\n",
    "img = torch.as_tensor(result.orig_img)\n",
    "img = img.permute(2, 0, 1)\n",
    "img = img.float() / 255.0\n",
    "H_img, W_img = img.shape[-2:]\n",
    "\n",
    "masks = torch.as_tensor(result.masks.data).float()\n",
    "N, Hm, Wm = masks.shape\n",
    "\n",
    "masks = F.interpolate(\n",
    "    masks.unsqueeze(1),  # [N, 1, Hm, Wm]\n",
    "    size=(H_img, W_img),\n",
    "    mode=\"nearest\",  # preserves binary mask\n",
    ").squeeze(1)\n",
    "\n",
    "# ---- 3. Bounding boxes from masks ----\n",
    "boxes = masks_to_boxes(masks)  # [N, 4], (x1, y1, x2, y2)\n",
    "\n",
    "# ---- 4. Create N masked images ----\n",
    "# img: [3, H, W] -> [N, 3, H, W] (viewed via expand)\n",
    "img_batch = img.unsqueeze(0).expand(N, -1, -1, -1)  # [N, 3, H, W]\n",
    "\n",
    "# masks: [N, H, W] -> [N, 1, H, W]\n",
    "masks_batched = masks.unsqueeze(1)  # [N, 1, H, W]\n",
    "\n",
    "# Elementwise multiply to zero-out background\n",
    "masked_imgs = img_batch * masks_batched  # [N, 3, H, W]\n",
    "\n",
    "batch_idx = torch.arange(N).float().unsqueeze(1)  # [N, 1]\n",
    "rois = torch.cat([batch_idx, boxes], dim=1)  # [N, 5] = [batch_idx, x1, y1, x2, y2]\n",
    "\n",
    "crops = roi_align(\n",
    "    masked_imgs,  # [N, 3, H_img, W_img]\n",
    "    rois,  # [N, 5]\n",
    "    output_size=(224, 224),\n",
    "    spatial_scale=1.0,\n",
    "    aligned=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aa0344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "for idx, crop in enumerate(crops):\n",
    "    print(idx)\n",
    "    rgb = crop[[2, 1, 0], ...]\n",
    "    # Image()\n",
    "    display(to_pil_image(rgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b77d1b0",
   "metadata": {},
   "source": [
    "# DINOv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6aa527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"facebook/dinov3-vitl16-pretrain-sat493m\",\n",
    "    # \"facebook/dinov3-convnext-large-pretrain-lvd1689m\",\n",
    "    # \"facebook/dinov3-vitb16-pretrain-lvd1689m\",\n",
    "    device_map=\"auto\",\n",
    "    # \"facebook/dinov3-vits16-pretrain-lvd1689m\", device_map=\"auto\"\n",
    "    # \"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\", device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0736ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_utils import load_image\n",
    "\n",
    "dino_inputs = [load_image(to_pil_image(crop[[2, 1, 0], ...])) for crop in crops]\n",
    "processor = AutoImageProcessor.from_pretrained(model.name_or_path)\n",
    "dino_pixels = processor(\n",
    "    images=dino_inputs,\n",
    "    # do_rescale=False,\n",
    "    # do_resize=False,\n",
    "    return_tensors=\"pt\",\n",
    "    device=model.device,\n",
    ")\n",
    "\n",
    "ref_pixels = processor(\n",
    "    images=[\n",
    "        load_image(str(path.with_name(f\"{path.stem}_obj.png\"))) for path in ref_paths\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    "    device=model.device,\n",
    ")\n",
    "\n",
    "crops[0][0], dino_pixels.pixel_values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    dino_input = model(**dino_pixels)\n",
    "    ref_input = model(**ref_pixels)\n",
    "    f6 = F.normalize(dino_input.pooler_output, dim=1)  # [6, 768]\n",
    "    f3 = F.normalize(ref_input.pooler_output, dim=1)  # [3, 768]\n",
    "\n",
    "    # Cosine similarity matrix: [3, 6]\n",
    "    M = F.cosine_similarity(\n",
    "        dino_input.pooler_output.unsqueeze(1),  # [6, 1, 768]\n",
    "        ref_input.pooler_output.unsqueeze(0),  # [1, 3, 768]\n",
    "        dim=-1,\n",
    "    )  # 9, 3\n",
    "    M = M.mean(dim=1)\n",
    "\n",
    "    # sim_mean_of_similarities = M.mean(dim=1)\n",
    "\n",
    "    # # For each of the 3 embeddings, find the index (0â€“5) with highest similarity\n",
    "    # best_indices = sim.argmax(dim=1)  # [3]\n",
    "\n",
    "    # print(best_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da506a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "providers = [\n",
    "    \"CUDAExecutionProvider\",\n",
    "    (\n",
    "        \"CoreMLExecutionProvider\",\n",
    "        {\n",
    "            \"ModelFormat\": \"MLProgram\",\n",
    "            \"RequireStaticInputShapes\": \"1\",\n",
    "            \"AllowLowPrecisionAccumulationOnGPU\": \"1\",\n",
    "        },\n",
    "    ),\n",
    "    \"CPUExecutionProvider\",\n",
    "]\n",
    "\n",
    "ORT_TYPE_TO_NUMPY = {\n",
    "    \"tensor(float)\": np.float32,\n",
    "    \"tensor(uint8)\": np.uint8,\n",
    "    \"tensor(int8)\": np.int8,\n",
    "    \"tensor(uint16)\": np.uint16,\n",
    "    \"tensor(int16)\": np.int16,\n",
    "    \"tensor(int32)\": np.int32,\n",
    "    \"tensor(int64)\": np.int64,\n",
    "    \"tensor(double)\": np.float64,\n",
    "    \"tensor(bool)\": bool,\n",
    "    \"tensor(float16)\": np.float16,\n",
    "}\n",
    "\n",
    "\n",
    "def get_ort_session_device_type(session: ort.InferenceSession) -> str:\n",
    "    # get_providers() returns e.g. [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "    provider = session.get_providers()[0]\n",
    "    # strip the common suffix and lower-case\n",
    "    return provider[: provider.index(\"ExecutionProvider\")].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa971d",
   "metadata": {},
   "source": [
    "# BEN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e78eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(MODEL_DIR / \"ben2\" / \"fp16.onnx\", providers=providers)\n",
    "io_binding = session.io_binding()\n",
    "input_node = session.get_inputs()[0]\n",
    "output_node = session.get_outputs()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfafa617",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_type = get_ort_session_device_type(session)\n",
    "if device_type == \"coreml\":\n",
    "    device_type = \"cpu\"\n",
    "\n",
    "b, c, h, w = input_node.shape\n",
    "input_batch = np.empty([b, c, h, w], dtype=np.float32)\n",
    "\n",
    "if device_type != \"cpu\":\n",
    "    input_ortvalue = ort.OrtValue.ortvalue_from_shape_and_type(\n",
    "        input_node.shape, ORT_TYPE_TO_NUMPY[input_node.type], device_type\n",
    "    )\n",
    "    io_binding.bind_ortvalue_input(input_node.name, input_ortvalue)\n",
    "else:\n",
    "    io_binding.bind_cpu_input(input_node.name, input_batch)\n",
    "\n",
    "output_ortvalue = ort.OrtValue.ortvalue_from_shape_and_type(\n",
    "    output_node.shape, ORT_TYPE_TO_NUMPY[output_node.type], device_type\n",
    ")\n",
    "io_binding.bind_ortvalue_output(output_node.name, output_ortvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df80104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import batched\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 1\n",
    "ref_paths = list(Path(\"ref\").iterdir())\n",
    "\n",
    "input_rgb_list: list[np.ndarray | None] = [None] * batch_size\n",
    "\n",
    "for batch_paths in batched(tqdm(ref_paths), batch_size):\n",
    "    for idx, img_path in enumerate(batch_paths):\n",
    "        input_rgb_list[idx] = cv2.imread(str(img_path), cv2.IMREAD_COLOR_RGB)\n",
    "        resized_image = cv2.resize(input_rgb_list[idx], (1024, 1024))\n",
    "        chw_image = resized_image.transpose(2, 0, 1)\n",
    "        np.divide(chw_image, np.iinfo(np.uint8).max, out=input_batch[idx])\n",
    "\n",
    "    # Inference\n",
    "    if device_type != \"cpu\":\n",
    "        # Update existing OrtValue's memory in-place (no re-alloc)\n",
    "        input_ortvalue.update_inplace(input_batch)\n",
    "    session.run_with_iobinding(io_binding)\n",
    "    outputs = output_ortvalue.numpy()\n",
    "\n",
    "    # Postprocess\n",
    "    for img_path, input_rgb, output in zip(batch_paths, input_rgb_list, outputs):\n",
    "        raw_mask = output.squeeze()\n",
    "        min_val = raw_mask.min()\n",
    "        max_val = raw_mask.max()\n",
    "\n",
    "        normalized_mask = (raw_mask - min_val) / (\n",
    "            max_val - min_val + np.finfo(np.float32).eps\n",
    "        )\n",
    "        normalized_mask *= np.iinfo(np.uint8).max\n",
    "\n",
    "        resized_mask = cv2.resize(\n",
    "            normalized_mask.astype(np.uint8),\n",
    "            dsize=(input_rgb.shape[1], input_rgb.shape[0]),\n",
    "        )\n",
    "\n",
    "        save_path = img_path.with_name(f\"{img_path.stem}_bg.png\")\n",
    "        cv2.imwrite(str(save_path), resized_mask, [cv2.IMWRITE_PNG_COMPRESSION, 9])\n",
    "\n",
    "        bgra_image = cv2.cvtColor(input_rgb, cv2.COLOR_RGB2BGRA)\n",
    "        bgra_image[:, :, 3] = resized_mask\n",
    "        save_path = img_path.with_name(f\"{img_path.stem}_obj.png\")\n",
    "        cv2.imwrite(str(save_path), bgra_image, [cv2.IMWRITE_PNG_COMPRESSION, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af3cd42",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.downloads import GITHUB_ASSETS_STEMS\n",
    "\n",
    "[i for i in GITHUB_ASSETS_STEMS if \"yoloe\" in i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
